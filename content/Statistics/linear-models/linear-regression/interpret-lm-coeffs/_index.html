---
weight: 1
title: "Interpretación de Regresiones Lineales"
output: html_document
aliases: /how-to-interpret-linear-models/
---



<p>La interpretación de los coeficientes en modelos lineales (generalizados) es más sutil de lo que puedes darte cuenta, y tiene consecuencias en cómo probamos hipótesis y reportamos hallazgos. Comenzaremos hablando de las interpretaciones <strong>marginales vs. condicionales</strong> de los parámetros del modelo.</p>
<p><img src="Linear_regression_image.jpg" /></p>
<p>En este ejemplo, modelamos la altura de las plantas en función de la altitud y la temperatura. Estas variables están correlacionadas de manera negativa: hace más frío a mayor altitud. Comenzamos simulando algunos datos para reflejar esto.</p>
<pre class="r"><code>library(mvtnorm)

# Specify the sample size
N &lt;- 1000

# Specify the correlation between altitude and temperature
rho &lt;- -0.4

# This line of code creates two correlated variables
X &lt;- rmvnorm(N, mean = c(0, 0), sigma = matrix(c(1, rho, rho, 1), 2, 2))

# Extract the first and second columns to vectors named temp and alt and plot
temp &lt;- X[, 1]
alt &lt;- X[, 2]
plot(temp, alt)</code></pre>
<p><img src="/Statistics/linear-models/linear-regression/interpret-lm-coeffs/_index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Ahora podemos simular algunos datos de altura de las plantas. Aquí decimos que la altura media de las plantas es 2 (cuando todas las demás variables son 0). A medida que la temperatura aumenta en una unidad (manteniendo la altitud constante), la media de la altura aumentará en 1 unidad (<code>beta[2] = 1</code>), y de manera similar, a medida que aumentas la altitud en 1 unidad (manteniendo la temperatura constante), la media de la altura disminuirá en 1 (<code>beta[3] = -1</code>). La altura sigue una distribución normal con esta media y una desviación estándar de 2.</p>
<pre class="r"><code>beta &lt;- c(2, 1, -1)
mu &lt;- beta[1] + beta[2] * temp + beta[3] * alt
height &lt;- rnorm(N, mu, sd = 2)</code></pre>
<p>Si utilizamos un modelo lineal para encontrar los coeficientes, obtenemos lo que esperamos, estimaciones muy cercanas a los valores reales.</p>
<pre class="r"><code>lm_both &lt;- lm(height ~ temp + alt)
data.frame(estimated = round(lm_both$coefficients, 2), true = beta)</code></pre>
<pre><code>##             estimated true
## (Intercept)      1.92    2
## temp             0.91    1
## alt             -0.97   -1</code></pre>
<p>La interpretación de estos coeficientes es que si mantienes todo lo demás en el modelo constante (es decir, la temperatura) y agregas 1 a la altitud, entonces la altura media estimada disminuirá en 1.09. Ten en cuenta que el coeficiente depende de las unidades en las que se mide la altitud. Si la altitud se mide en metros, entonces el coeficiente te dice qué sucede cuando subes 1 metro.</p>
<p>La intersección es el valor predicho cuando todas las demás variables se establecen en 0, lo cual a veces tiene sentido (aquí sería la altura de las plantas a nivel del mar y a 0 grados de temperatura). Otras veces, 0 no es un valor significativo, y si deseas interpretar la intersección, podría tener sentido reescalar tus otras variables para que su media sea 0. Si haces esto, entonces la intersección es el valor predicho cuando todas las demás variables están en su nivel medio.</p>
<p>¿Y si ahora tuviéramos un modelo solo con la temperatura?</p>
<pre class="r"><code>lm1 &lt;- lm(height ~ temp)
lm1$coefficients</code></pre>
<pre><code>## (Intercept)        temp 
##    1.939198    1.309778</code></pre>
<p>El coeficiente de temperatura ahora es <code>1.38</code>, ¿qué está pasando? La altitud es un predictor importante de la altura de las plantas, y parte de la información sobre la altitud está contenida en la temperatura (recuerda que están correlacionadas, por lo que a medida que la altitud aumenta, la temperatura disminuye). El modelo tiene en cuenta esto cambiando el efecto de la temperatura para tener en cuenta la información que contiene sobre la altitud. Observa que el coeficiente de temperatura está incorrecto en aproximadamente 0.4, que es la cantidad de correlación entre las variables.</p>
<p>Nota: Cuando los estadísticos hablan de esto, usan las palabras <strong>condicional</strong> y <strong>marginal</strong>. Condicional es el efecto de una variable cuando las demás se mantienen constantes (como en lm_both), mientras que marginal es el efecto global (como en lm1).
Nota: Si utilizas el código anterior para simular tus propios conjuntos de datos, obtendrás valores ligeramente diferentes para los coeficientes.</p>
<div id="pruebas-de-hipótesis" class="section level3">
<h3>Pruebas de hipótesis</h3>
<p>Esta distinción tiene muchas consecuencias tanto para el modelado como para las pruebas de hipótesis. Generemos algunos datos en los que la altitud predice la altura, y la temperatura no tiene (información adicional), y luego probemos la temperatura.</p>
<pre class="r"><code>mu &lt;- 2 - 1 * alt
height &lt;- rnorm(N, mu, sd = 2)

mod_temp &lt;- lm(height ~ temp)
summary(mod_temp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = height ~ temp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.9920 -1.3798  0.0109  1.5252  6.3231 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.98443    0.06889  28.804  &lt; 2e-16 ***
## temp         0.49575    0.07084   6.999 4.74e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.178 on 998 degrees of freedom
## Multiple R-squared:  0.04678,	Adjusted R-squared:  0.04583 
## F-statistic: 48.98 on 1 and 998 DF,  p-value: 4.739e-12</code></pre>
<pre class="r"><code>anova(mod_temp)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: height
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## temp        1  232.3 232.315   48.98 4.739e-12 ***
## Residuals 998 4733.6   4.743                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>La salida de este modelo nos está indicando que hay un efecto de la temperatura, aunque técnicamente no lo haya. No nos está dando información falsa si entendemos cómo interpretar los resultados del modelo. Debido a que la temperatura está correlacionada con la altitud, y hay un efecto de la altitud, cuando la altitud no está en el modelo, el modelo nos dice en general que hay un efecto de la temperatura que aumenta la altura en <code>0.45</code> (recuerda que la correlación fue <code>0.4</code>). Si nuestra hipótesis es “¿La altura de las plantas cambia con la temperatura?”, la respuesta es sí, a mayor temperatura, más altas son las plantas.</p>
<p>Pero, ¿qué pasa con la altitud? Sabemos que el efecto de la temperatura que observamos se debe a que está correlacionado con la altitud, la temperatura no predice directamente la altura. Si queremos saber si hay un efecto de la temperatura después de controlar la altitud (manteniendo la altitud constante, es decir, condicional), entonces ajustamos el modelo con la altitud y luego probamos la temperatura.</p>
<pre class="r"><code>mod_temp_alt &lt;- lm(height ~ alt + temp)
summary(mod_temp_alt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = height ~ alt + temp)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.460 -1.270 -0.004  1.425  5.757 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.96290    0.06266  31.327   &lt;2e-16 ***
## alt         -0.98466    0.06791 -14.499   &lt;2e-16 ***
## temp         0.09568    0.07007   1.365    0.172    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.98 on 997 degrees of freedom
## Multiple R-squared:  0.2128,	Adjusted R-squared:  0.2112 
## F-statistic: 134.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>anova(mod_temp_alt)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: height
##            Df Sum Sq Mean Sq  F value Pr(&gt;F)    
## alt         1 1049.3 1049.27 267.5949 &lt;2e-16 ***
## temp        1    7.3    7.31   1.8645 0.1724    
## Residuals 997 3909.3    3.92                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>El p-valor es aproximadamente <code>0.95</code>, por lo que no tenemos evidencia de un efecto de la temperatura después de controlar la altitud.</p>
<p>Nota: La distinción entre interpretaciones condicionales y marginales también es válida para modelos lineales generalizados y modelos mixtos.</p>
</div>
<div id="covariables-categóricas" class="section level3">
<h3>Covariables categóricas</h3>
<p>Cuando tenemos covariables categóricas (por ejemplo, tratamiento), hay varias formas de codificar el modelo, lo que dará diferentes interpretaciones para los coeficientes. Simulemos <code>120</code> puntos de datos con 40 en cada uno de los tres niveles de un tratamiento categórico.</p>
<pre class="r"><code>N &lt;- 120
# The effect of treatment
trt.n &lt;- rep(c(-1, 0, 1), N / 3)
mu &lt;- 2 + 1 * trt.n

# Labels for the treatment
treatment &lt;- factor(rep(c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;), N / 3)) # group labels

# Create, Y, a normally distributed response variable and plot against treatment
Y &lt;- rnorm(N, mu, sd = 2)
boxplot(Y ~ treatment)</code></pre>
<p><img src="/Statistics/linear-models/linear-regression/interpret-lm-coeffs/_index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Si introducimos el tratamiento como una covariable de la forma habitual, el modelo elegirá un tratamiento de referencia (aquí será “high” ya que los niveles se ordenan alfabéticamente), de modo que la intersección será la media de este grupo de referencia. Los demás coeficientes serán las diferencias entre los otros grupos y el grupo de referencia.</p>
<pre class="r"><code>cat_lm &lt;- lm(Y ~ treatment)
summary(cat_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ treatment)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8252 -1.1715 -0.0491  1.1781  4.4219 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    2.9838     0.2732  10.922  &lt; 2e-16 ***
## treatmentlow  -2.3107     0.3864  -5.981 2.47e-08 ***
## treatmentmed  -0.6599     0.3864  -1.708   0.0903 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.728 on 117 degrees of freedom
## Multiple R-squared:  0.245,	Adjusted R-squared:  0.2321 
## F-statistic: 18.98 on 2 and 117 DF,  p-value: 7.252e-08</code></pre>
<p>Entonces, el grupo “high” tiene una media de <code>2.65</code>, y la diferencia entre las medias del grupo “low” y el grupo “high” es de <code>-0.66</code>, y la diferencia entre el grupo “med” y el grupo “high” es de <code>-1.48</code>. Si deseas tener otro grupo como grupo de referencia, puedes usar <code>relevel</code> para recodificar tu factor de tratamiento.</p>
<pre class="r"><code>treatment &lt;- relevel(treatment, ref = &quot;low&quot;)
cat_lm &lt;- lm(Y ~ treatment)
summary(cat_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ treatment)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8252 -1.1715 -0.0491  1.1781  4.4219 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     0.6731     0.2732   2.464   0.0152 *  
## treatmenthigh   2.3107     0.3864   5.981 2.47e-08 ***
## treatmentmed    1.6509     0.3864   4.273 3.95e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.728 on 117 degrees of freedom
## Multiple R-squared:  0.245,	Adjusted R-squared:  0.2321 
## F-statistic: 18.98 on 2 and 117 DF,  p-value: 7.252e-08</code></pre>
<pre class="r"><code>boxplot(Y ~ treatment)</code></pre>
<p><img src="/Statistics/linear-models/linear-regression/interpret-lm-coeffs/_index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Ahora la intersección es la media del grupo “low”, y todos los demás coeficientes son las diferencias entre el grupo “low” y los demás. Otra cosa que puedes hacer es poner <code>-1</code> en el modelo para eliminar la intersección y tener solo las medias de cada grupo como coeficientes.</p>
<pre class="r"><code>cat_lm &lt;- lm(Y ~ treatment - 1)
summary(cat_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ treatment - 1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8252 -1.1715 -0.0491  1.1781  4.4219 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## treatmentlow    0.6731     0.2732   2.464   0.0152 *  
## treatmenthigh   2.9838     0.2732  10.922  &lt; 2e-16 ***
## treatmentmed    2.3240     0.2732   8.507 6.87e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.728 on 117 degrees of freedom
## Multiple R-squared:  0.6282,	Adjusted R-squared:  0.6187 
## F-statistic: 65.91 on 3 and 117 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Ahora, los tres coeficientes son las medias de los grupos.</p>
<p><strong>Contraste de los coeficientes</strong> También podemos ver los contrastes; estos son las diferencias entre todos los pares de grupos. Carga el paquete <a href="https://cran.r-project.org/web/packages/multcomp/index.html">multcomp</a> y utiliza <code>glht</code> (hipótesis lineales generales) para examinar todas las diferencias de pares.</p>
<pre class="r"><code>library(multcomp)

cont &lt;- glht(cat_lm, linfct = mcp(treatment = &quot;Tukey&quot;))

summary(cont)</code></pre>
<pre><code>## 
## 	 Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Y ~ treatment - 1)
## 
## Linear Hypotheses:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## high - low == 0   2.3107     0.3864   5.981  &lt; 1e-04 ***
## med - low == 0    1.6509     0.3864   4.273 0.000119 ***
## med - high == 0  -0.6599     0.3864  -1.708 0.206487    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>Cada línea de esta salida compara dos grupos entre sí. La primera línea, por ejemplo, compara el grupo “high” con el grupo “low”. Por lo tanto, la diferencia entre las medias de los grupos “high” y “low” es de 1.84. Los valores de p y los intervalos de confianza proporcionados por <code>glht</code> controlan las pruebas múltiples, lo cual es útil. Si deseas ver los intervalos de confianza para las diferencias entre los grupos.</p>
<pre class="r"><code>confint(cont)</code></pre>
<pre><code>## 
## 	 Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Y ~ treatment - 1)
## 
## Quantile = 2.3736
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                 Estimate lwr     upr    
## high - low == 0  2.3107   1.3937  3.2278
## med - low == 0   1.6509   0.7338  2.5679
## med - high == 0 -0.6599  -1.5769  0.2572</code></pre>
<p>Nota: En un modelo con múltiples covariables, las mismas reglas siguen aplicándose en cuanto a las interpretaciones condicionales y marginales de los coeficientes.</p>
<p><strong>Interpretación de los coeficientes en modelos lineales generalizados</strong> En los modelos lineales, la interpretación de los parámetros del modelo es lineal, como se discutió anteriormente. Para los modelos lineales generalizados, ahora lee la página de tutoriales sobre <a href="/statistics/glms/interpret-glm-coeffs/">la interpretación de los coeficientes en esos modelos</a>.</p>
<p><strong>Autor</strong>: Gordana Popovic
<strong>Año</strong>: 2016
<strong>Última actualización</strong>: Jun. 2023</p>
</div>
